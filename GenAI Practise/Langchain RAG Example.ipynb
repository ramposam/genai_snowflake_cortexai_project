{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc4276-a77e-4f17-b4dc-5eacdcd3a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web URL RAG System with ChromaDB and GPT-4o-mini\n",
    "# Requirements: pip install langchain langchain-community langchain-openai chromadb sentence-transformers beautifulsoup4 requests\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Additional imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import chromadb\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class WebRAGChromaSystem:\n",
    "    def __init__(self, \n",
    "                 openai_api_key: str,\n",
    "                 embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 persist_directory: str = \"./chroma_db\",\n",
    "                 collection_name: str = \"web_documents\"):\n",
    "        \"\"\"\n",
    "        Initialize Web RAG system with ChromaDB and GPT-4o-mini\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key\n",
    "            embedding_model: HuggingFace embedding model\n",
    "            persist_directory: ChromaDB persistence directory\n",
    "            collection_name: ChromaDB collection name\n",
    "        \"\"\"\n",
    "        # Set OpenAI API key\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "        \n",
    "        self.persist_directory = Path(persist_directory)\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"üîÑ Initializing embedding model...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model,\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        print(\"üîÑ Initializing ChatGPT-4o-mini...\")\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        self.rag_chain = None\n",
    "        \n",
    "        print(\"‚úÖ Web RAG system initialized successfully\")\n",
    "    \n",
    "    def load_web_urls(self, urls: List[str]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load content from web URLs using LangChain WebBaseLoader\n",
    "        \n",
    "        Args:\n",
    "            urls: List of URLs to scrape\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects\n",
    "        \"\"\"\n",
    "        print(f\"üåê Loading content from {len(urls)} URLs...\")\n",
    "        \n",
    "        all_documents = []\n",
    "        \n",
    "        for url in urls:\n",
    "            try:\n",
    "                print(f\"üì° Scraping: {url}\")\n",
    "                \n",
    "                # Use WebBaseLoader for better content extraction\n",
    "                loader = WebBaseLoader(\n",
    "                    web_paths=[url],\n",
    "                    bs_kwargs={\n",
    "                        \"parse_only\": BeautifulSoup.SoupStrainer([\n",
    "                            \"article\", \"main\", \"div\", \"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"\n",
    "                        ])\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                documents = loader.load()\n",
    "                \n",
    "                # Add URL to metadata\n",
    "                for doc in documents:\n",
    "                    doc.metadata[\"source_url\"] = url\n",
    "                    doc.metadata[\"source_type\"] = \"web\"\n",
    "                \n",
    "                all_documents.extend(documents)\n",
    "                print(f\"‚úÖ Loaded {len(documents)} documents from {url}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading {url}: {e}\")\n",
    "                # Try alternative scraping method\n",
    "                try:\n",
    "                    alt_doc = self._alternative_scrape(url)\n",
    "                    if alt_doc:\n",
    "                        all_documents.append(alt_doc)\n",
    "                except Exception as alt_e:\n",
    "                    print(f\"‚ùå Alternative scraping also failed: {alt_e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Total loaded: {len(all_documents)} documents\")\n",
    "        return all_documents\n",
    "    \n",
    "    def _alternative_scrape(self, url: str) -> Optional[Document]:\n",
    "        \"\"\"Alternative web scraping method using requests + BeautifulSoup\"\"\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        if text.strip():\n",
    "            return Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"source\": url,\n",
    "                    \"source_url\": url,\n",
    "                    \"source_type\": \"web\",\n",
    "                    \"title\": soup.title.string if soup.title else \"No Title\"\n",
    "                }\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def split_documents(self, documents: List[Document], \n",
    "                       chunk_size: int = 1000, \n",
    "                       chunk_overlap: int = 200) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split documents into chunks optimized for web content\n",
    "        \n",
    "        Args:\n",
    "            documents: List of documents to split\n",
    "            chunk_size: Maximum chunk size\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of document chunks\n",
    "        \"\"\"\n",
    "        print(\"‚úÇÔ∏è Splitting documents into chunks...\")\n",
    "        \n",
    "        # Web-optimized text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\n",
    "                \"\\n\\n\\n\",    # Multiple newlines\n",
    "                \"\\n\\n\",      # Double newlines\n",
    "                \"\\n\",        # Single newlines\n",
    "                \". \",        # Sentences\n",
    "                \"! \",        # Exclamations\n",
    "                \"? \",        # Questions\n",
    "                \"; \",        # Semicolons\n",
    "                \", \",        # Commas\n",
    "                \" \",         # Spaces\n",
    "                \"\"           # Characters\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Add chunk metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata[\"chunk_id\"] = i\n",
    "            chunk.metadata[\"chunk_size\"] = len(chunk.page_content)\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "    def create_chroma_vectorstore(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Create ChromaDB vector store from documents\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document chunks\n",
    "        \"\"\"\n",
    "        print(\"üîÆ Creating ChromaDB vector store...\")\n",
    "        \n",
    "        try:\n",
    "            # Create or load ChromaDB vector store\n",
    "            self.vectorstore = Chroma.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=self.embeddings,\n",
    "                collection_name=self.collection_name,\n",
    "                persist_directory=str(self.persist_directory)\n",
    "            )\n",
    "            \n",
    "            # Persist the database\n",
    "            self.vectorstore.persist()\n",
    "            \n",
    "            print(f\"‚úÖ ChromaDB vector store created with {len(documents)} documents\")\n",
    "            print(f\"üíæ Persisted to: {self.persist_directory}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_existing_vectorstore(self) -> bool:\n",
    "        \"\"\"Load existing ChromaDB vector store\"\"\"\n",
    "        try:\n",
    "            if (self.persist_directory / \"chroma.sqlite3\").exists():\n",
    "                print(\"üìÇ Loading existing ChromaDB vector store...\")\n",
    "                \n",
    "                self.vectorstore = Chroma(\n",
    "                    collection_name=self.collection_name,\n",
    "                    embedding_function=self.embeddings,\n",
    "                    persist_directory=str(self.persist_directory)\n",
    "                )\n",
    "                \n",
    "                print(\"‚úÖ Existing vector store loaded successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå No existing ChromaDB found\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading existing vector store: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def setup_retriever(self, search_type: str = \"similarity\", k: int = 4) -> None:\n",
    "        \"\"\"\n",
    "        Setup document retriever from vector store\n",
    "        \n",
    "        Args:\n",
    "            search_type: Type of search (\"similarity\", \"mmr\")\n",
    "            k: Number of documents to retrieve\n",
    "        \"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Vector store not created. Please create vector store first.\")\n",
    "        \n",
    "        print(f\"üîç Setting up retriever (type: {search_type}, k: {k})...\")\n",
    "        \n",
    "        # Create retriever\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs={\"k\": k}\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Retriever setup complete\")\n",
    "    \n",
    "    def create_rag_chain(self) -> None:\n",
    "        \"\"\"Create RAG chain with custom prompt template\"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Retriever not setup. Please setup retriever first.\")\n",
    "        \n",
    "        print(\"üîó Creating RAG chain with GPT-4o-mini...\")\n",
    "        \n",
    "        # Custom prompt template for web content RAG\n",
    "        prompt_template = \"\"\"You are a helpful assistant that answers questions based on web content provided as context.\n",
    "Use the following pieces of context from web pages to answer the question. If you don't know the answer based on the context, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context from web pages:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Provide a comprehensive answer based on the context\n",
    "- If relevant, mention which website(s) the information comes from\n",
    "- Be specific and cite details from the context\n",
    "- If the context doesn't contain enough information, clearly state what's missing\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        PROMPT = PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # Create the RAG chain using LCEL (LangChain Expression Language)\n",
    "        self.rag_chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | self._format_docs,\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | PROMPT\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ RAG chain created successfully\")\n",
    "    \n",
    "    def _format_docs(self, docs: List[Document]) -> str:\n",
    "        \"\"\"Format retrieved documents for context\"\"\"\n",
    "        formatted_docs = []\n",
    "        \n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            source_url = doc.metadata.get(\"source_url\", \"Unknown URL\")\n",
    "            title = doc.metadata.get(\"title\", \"\")\n",
    "            \n",
    "            formatted_doc = f\"Source {i} ({source_url}):\\n\"\n",
    "            if title and title != \"No Title\":\n",
    "                formatted_doc += f\"Title: {title}\\n\"\n",
    "            formatted_doc += f\"Content: {doc.page_content}\\n\"\n",
    "            \n",
    "            formatted_docs.append(formatted_doc)\n",
    "        \n",
    "        return \"\\n\" + \"-\"*50 + \"\\n\".join(formatted_docs)\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query the RAG system\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        if not self.rag_chain:\n",
    "            raise ValueError(\"RAG chain not created. Please create RAG chain first.\")\n",
    "        \n",
    "        print(f\"ü§î Processing question: {question}\")\n",
    "        \n",
    "        try:\n",
    "            # Get relevant documents for context\n",
    "            retrieved_docs = self.retriever.invoke(question)\n",
    "            \n",
    "            # Generate answer using RAG chain\n",
    "            answer = self.rag_chain.invoke(question)\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_documents\": retrieved_docs,\n",
    "                \"num_sources\": len(retrieved_docs)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"Error processing question: {str(e)}\",\n",
    "                \"source_documents\": [],\n",
    "                \"num_sources\": 0\n",
    "            }\n",
    "    \n",
    "    def process_urls_pipeline(self, urls: List[str], \n",
    "                             chunk_size: int = 1000, \n",
    "                             chunk_overlap: int = 200,\n",
    "                             force_recreate: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Complete pipeline to process URLs and setup RAG system\n",
    "        \n",
    "        Args:\n",
    "            urls: List of URLs to process\n",
    "            chunk_size: Chunk size for text splitting\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            force_recreate: Force recreation of vector store\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Starting web URL processing pipeline...\")\n",
    "        \n",
    "        # Try to load existing vector store\n",
    "        if not force_recreate and self.load_existing_vectorstore():\n",
    "            print(\"üìã Using existing vector store\")\n",
    "        else:\n",
    "            # Load and process web content\n",
    "            documents = self.load_web_urls(urls)\n",
    "            if not documents:\n",
    "                raise ValueError(\"No documents could be loaded from the provided URLs\")\n",
    "            \n",
    "            chunks = self.split_documents(documents, chunk_size, chunk_overlap)\n",
    "            self.create_chroma_vectorstore(chunks)\n",
    "        \n",
    "        # Setup retriever and RAG chain\n",
    "        self.setup_retriever(search_type=\"similarity\", k=4)\n",
    "        self.create_rag_chain()\n",
    "        \n",
    "        print(\"üéâ Pipeline complete! RAG system ready for queries.\")\n",
    "    \n",
    "    def interactive_chat(self) -> None:\n",
    "        \"\"\"Interactive chat interface\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ü§ñ Web RAG Chat Interface with GPT-4o-mini\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Commands:\")\n",
    "        print(\"  'quit', 'exit', 'q' - Exit the chat\")\n",
    "        print(\"  'sources' - Show sources for the last answer\")\n",
    "        print(\"  'stats' - Show vector store statistics\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        last_result = None\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nüí¨ Your question: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if question.lower() == 'sources' and last_result:\n",
    "                print(f\"\\nüìö Sources for last answer ({last_result['num_sources']} documents):\")\n",
    "                for i, doc in enumerate(last_result['source_documents'], 1):\n",
    "                    source_url = doc.metadata.get(\"source_url\", \"Unknown\")\n",
    "                    print(f\"\\n{i}. {source_url}\")\n",
    "                    print(f\"   Content preview: {doc.page_content[:150]}...\")\n",
    "                continue\n",
    "            \n",
    "            if question.lower() == 'stats':\n",
    "                if self.vectorstore:\n",
    "                    try:\n",
    "                        collection = self.vectorstore._collection\n",
    "                        count = collection.count()\n",
    "                        print(f\"\\nüìä Vector Store Stats:\")\n",
    "                        print(f\"   Total documents: {count}\")\n",
    "                        print(f\"   Collection name: {self.collection_name}\")\n",
    "                        print(f\"   Persist directory: {self.persist_directory}\")\n",
    "                    except:\n",
    "                        print(\"üìä Unable to retrieve stats\")\n",
    "                continue\n",
    "            \n",
    "            if not question:\n",
    "                continue\n",
    "            \n",
    "            # Process the question\n",
    "            result = self.query(question)\n",
    "            last_result = result\n",
    "            \n",
    "            print(f\"\\nü§ñ GPT-4o-mini Answer:\")\n",
    "            print(f\"{result['answer']}\")\n",
    "            \n",
    "            if result['num_sources'] > 0:\n",
    "                print(f\"\\nüìñ Based on {result['num_sources']} source(s). Type 'sources' to see details.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate the Web RAG system\"\"\"\n",
    "    print(\"üîß Web RAG System with ChromaDB and GPT-4o-mini\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get OpenAI API key\n",
    "    api_key = input(\"üîë Enter your OpenAI API key: \").strip()\n",
    "    if not api_key:\n",
    "        print(\"‚ùå OpenAI API key is required!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the system\n",
    "    try:\n",
    "        rag_system = WebRAGChromaSystem(\n",
    "            openai_api_key=api_key,\n",
    "            persist_directory=\"./web_chroma_db\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing system: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get URLs from user\n",
    "    print(\"\\nüì° Enter URLs to scrape (one per line, press Enter twice to finish):\")\n",
    "    urls = []\n",
    "    while True:\n",
    "        url = input(\"URL: \").strip()\n",
    "        if not url:\n",
    "            break\n",
    "        if url.startswith(('http://', 'https://')):\n",
    "            urls.append(url)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Please enter a valid URL starting with http:// or https://\")\n",
    "    \n",
    "    if not urls:\n",
    "        print(\"‚ùå No valid URLs provided!\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Process URLs and setup RAG system\n",
    "        rag_system.process_urls_pipeline(\n",
    "            urls=urls,\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            force_recreate=False\n",
    "        )\n",
    "        \n",
    "        # Start interactive chat\n",
    "        rag_system.interactive_chat()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Quick test function\n",
    "def quick_test():\n",
    "    \"\"\"Quick test with sample URLs\"\"\"\n",
    "    # Sample URLs for testing\n",
    "    test_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    ]\n",
    "    \n",
    "    api_key = input(\"üîë Enter OpenAI API key for quick test: \").strip()\n",
    "    \n",
    "    if api_key:\n",
    "        rag_system = WebRAGChromaSystem(openai_api_key=api_key)\n",
    "        rag_system.process_urls_pipeline(test_urls)\n",
    "        \n",
    "        # Test query\n",
    "        result = rag_system.query(\"What is artificial intelligence?\")\n",
    "        print(f\"\\nü§ñ Answer: {result['answer']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    choice = input(\"Choose mode:\\n1. Full interactive mode\\n2. Quick test\\nChoice (1/2): \").strip()\n",
    "    \n",
    "    if choice == \"2\":\n",
    "        quick_test()\n",
    "    else:\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
