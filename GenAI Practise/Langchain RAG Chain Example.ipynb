{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "0579e3bf-ac3b-41ed-bcb0-1d2d226d646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_file(file_name,current_dir=True):\n",
    "    if current_dir:\n",
    "        dir_path = os.path.dirname(os.getcwd())\n",
    "        file_path = os.path.join(dir_path,file_name)\n",
    "    else:\n",
    "        file_path = file_name\n",
    "        \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b1ed370b-383c-4ba4-a508-721322ff69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Analysis_of_Vote_Share_and_Margin_of_Victory_of_Winners_Andhra_Pradesh_Assembly_2024_Finalver_English.pdf\"\n",
    "docs =  load_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "aa0f8727-fb37-4c93-97ea-1847742cc837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2024-07-15T10:14:47+05:30', 'author': 'Navin Moni', 'moddate': '2024-07-15T10:14:47+05:30', 'title': 'Microsoft Word - Analysis of Vote Share and Margin of Victory of Winners Andhra Pradesh Assembly 2024.Finalver.English', 'source': 'C:\\\\Users\\\\Asus\\\\PycharmProjects\\\\GenAIPractise\\\\Analysis_of_Vote_Share_and_Margin_of_Victory_of_Winners_Andhra_Pradesh_Assembly_2024_Finalver_English.pdf', 'total_pages': 65, 'page': 20, 'page_label': '21'}, page_content=\"Page 21 of 65 \\n \\nVote Share and Representativeness of Winners in Andhra Pradesh Assembly Elections \\nS.No. Winner Party Constituency \\nTotal \\nRegistered \\nVoters  \\nTotal Valid \\nVotes in The \\nConstituency \\nTotal Votes \\nPolled for \\nWinner \\n% of Votes \\nShare  \\n% of \\nrepresentativeness \\n% of Voters ' \\nTurnout  \\n1 Nara Lokesh Telugu Desam Mangalagiri 286552 253830 167710 66.07% 58.53% 88.58% \\n2 Dr. Nimmala Ramanaidu Telugu Desam Palacole 193463 163213 113114 69.30% 58.47% 84.36% \\n3 Konidala Pawan Kalyan Janasena Party Pithapuram 230188 207169 134394 64.87% 58.38% 90.00% \\n4 Bommidi Narayana Nayakar Janasena Party Narasapuram 168259 145418 94116 64.72% 55.94% 86.43% \\n5 Arimilli Radha Krishna Telugu Desam Tanuku 233178 194768 129547 66.51% 55.56% 83.53% \\n6 Kagitha Krishnaprasad Telugu Desam Pedana 165828 149961 91394 60.95% 55.11% 90.43% \\n7 Venigandla Ramu Telugu Desam Gudivada 200876 171604 109980 64.09% 54.75% 85.43% \\n8 Chandrababu Naidu Nara Telugu Desam Kuppam 223306 203367 121929 59.96% 54.60% 91.07% \\n9 Bolisetty Srinivas Janasena Party Tadepalligudem 214049 178049 116443 65.40% 54.40% 83.18% \\n10 Konathala Ramakrishna Janasena Party Anakapalle 211927 172778 115126 66.63% 54.32% 81.53% \\n11 Kollu. Ravindra Telugu Desam Machilipatnam 193658 164839 105044 63.73% 54.24% 85.12% \\n12 Bandaru Satyananda Rao Telugu Desam Kothapeta 248484 217629 134286 61.70% 54.04% 87.58% \\n13 Buddhaprasad Mandali Janasena Party Avanigadda 211086 186445 113460 60.85% 53.75% 88.33% \\n14 Gali Bhanu Prakash Telugu Desam Nagari 201587 178530 107797 60.38% 53.47% 88.56% \\n15 Kamineni Srinivas Bharatiya Janata Party Kaikalur 204423 180991 109280 60.38% 53.46% 88.54% \\n16 Jogeswara Rao.V Telugu Desam Mandapeta 217714 194046 116309 59.94% 53.42% 89.13% \\n17 Tenali Sravan Kumar Telugu Desam Tadikonda 205133 184100 109585 59.52% 53.42% 89.75% \\n18 Sundarapu Vijay Kumar Janasena Party Yelamanchili 205067 179427 109443 61.00% 53.37% 87.50% \\n19 Dharmaraju Patsamatla Janasena Party Ungutur 205530 182623 108894 59.63% 52.98% 88.85% \\n20 Kanumuru Raghu Rama \\nKrishna Raju (R R R) Telugu Desam Undi 223101 195472 116902 59.80% 52.40% 87.62% \\n21 Amilineni Surendra Babu Telugu Desam Kalyandurg 227557 205841 118878 57.75% 52.24% 90.46% \\n22 Ramanjaneyulu Pulaparthi \\n(Anjibabu) Janasena Party Bhimavaram 251384 203983 130424 63.94% 51.88% 81.14% \\n23 Lokam Naga Madhavi Janasena Party Nellimarla 212523 190314 109915 57.75% 51.72% 89.55%\")"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(docs[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79012ef-cc71-4256-9f4d-145e1952b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_chunks(documents):\n",
    "    #Define markdown headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    separators =[\" \\n \\n \\n \",\" \\n \\n \",\" \\n \",\".  \\n\",\". \\n\",\". \"]\n",
    "    \n",
    "    # Initialize markdown splitter\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False\n",
    "    )\n",
    "    \n",
    "    # Initialize recursive character splitter as backup\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=separators\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        try:\n",
    "            # Try markdown splitting first\n",
    "            md_chunks = markdown_splitter.split_text(doc.page_content)\n",
    "            if md_chunks:\n",
    "                # Convert to Document objects\n",
    "                for chunk in md_chunks:\n",
    "                    chunk.metadata=doc.metadata\n",
    "                    chunks = text_splitter.split_documents([chunk])\n",
    "                    all_chunks.extend(chunks)\n",
    "            else:\n",
    "                # Fallback to recursive character splitting\n",
    "                chunks = text_splitter.split_documents([doc])\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Markdown splitting failed for a document: {e}\")\n",
    "            return []\n",
    "            # Fallback to recursive character splitting\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            all_chunks.extend(chunks)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a535e806-7034-4066-8ba3-f0d10efd4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = get_chunks(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "db24e88c-68ed-4ef1-b72d-326824969d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "bc684b05-8dfa-4d0e-9f51-30343240460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "from psycopg2.extras import execute_batch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BATCH_SIZE = 100  # Process embeddings in batches\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "class VectorEmbeddingsProcessor:\n",
    "    def __init__(self, connection_string: str,table_name, embedding_model: str = \"text-embedding-3-small\"):\n",
    "        self.connection_string = connection_string\n",
    "        self.embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "        self.table_name = table_name\n",
    "        \n",
    "    @contextmanager\n",
    "    def get_db_connection(self):\n",
    "        \"\"\"Context manager for database connections with proper cleanup\"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = psycopg2.connect(self.connection_string)\n",
    "            yield conn\n",
    "        except Exception as e:\n",
    "            if conn:\n",
    "                conn.rollback()\n",
    "            logger.error(f\"Database connection error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    \n",
    "    def sanitize_filename(self, filename: str) -> str:\n",
    "        \"\"\"Sanitize filename for database storage\"\"\"\n",
    "        base_name = os.path.basename(filename)\n",
    "        return re.sub(r'[^a-zA-Z0-9_.-]', '_', base_name)\n",
    "    \n",
    "    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for a batch of texts with retry logic\"\"\"\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                # Use embed_documents for batch processing (more efficient)\n",
    "                return self.embeddings.embed_documents(texts)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Embedding generation attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    raise\n",
    "                \n",
    "    def prepare_batch_data(self, chunks: List[Document]) -> List[tuple]:\n",
    "        \"\"\"Prepare data for batch insertion\"\"\"\n",
    "        batch_data = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            file_name = self.sanitize_filename(chunk.metadata.get(\"source\", \"unknown\"))\n",
    "            content = chunk.page_content\n",
    "            metadata_json = json.dumps(chunk.metadata)\n",
    "            \n",
    "            batch_data.append((file_name, content, metadata_json))\n",
    "            \n",
    "        return batch_data\n",
    "    \n",
    "    def create_table_if_not_exists(self, cursor):\n",
    "        \"\"\"Create the embeddings table if it doesn't exist\"\"\"\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.table_name} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            file_name VARCHAR(255) NOT NULL,\n",
    "            chunk TEXT NOT NULL,\n",
    "            embedding vector(1536),  -- Adjust dimension based on your model\n",
    "            metadata JSONB,\n",
    "            UNIQUE(file_name, chunk)  -- Prevent duplicates\n",
    "        );\n",
    "        \n",
    "        -- Create indexes for better query performance\n",
    "        CREATE INDEX IF NOT EXISTS idx_document_embeddings_file_name \n",
    "            ON public.t_document_embeddings(file_name);\n",
    "        CREATE INDEX IF NOT EXISTS idx_document_embeddings_embedding \n",
    "            ON public.t_document_embeddings USING ivfflat (embedding vector_cosine_ops);\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "    \n",
    "    def insert_embeddings_batch(self, cursor, batch_data: List[tuple], embeddings: List[List[float]]):\n",
    "        \"\"\"Insert embeddings in batch with upsert logic\"\"\"\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {self.table_name} (file_name, chunk, embedding, metadata)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        ON CONFLICT (file_name, chunk) \n",
    "        DO UPDATE SET \n",
    "            embedding = EXCLUDED.embedding,\n",
    "            metadata = EXCLUDED.metadata\n",
    "        \"\"\"\n",
    "        \n",
    "        # Combine batch data with embeddings\n",
    "        full_batch_data = [\n",
    "            (file_name, chunk, embedding, metadata)\n",
    "            for (file_name, chunk, metadata), embedding in zip(batch_data, embeddings)\n",
    "        ]\n",
    "        \n",
    "        # Use executemany instead of execute_batch for better compatibility\n",
    "        cursor.executemany(insert_query, full_batch_data)\n",
    "    \n",
    "    def process_documents(self, all_chunks: List[Document]) -> None:\n",
    "        \"\"\"Main method to process documents and store embeddings\"\"\"\n",
    "        if not all_chunks:\n",
    "            logger.warning(\"No chunks provided for processing\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Processing {len(all_chunks)} document chunks\")\n",
    "        \n",
    "        with self.get_db_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Create table and indexes if they don't exist\n",
    "            self.create_table_if_not_exists(cursor)\n",
    "            conn.commit()\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in range(0, len(all_chunks), BATCH_SIZE):\n",
    "                batch_chunks = all_chunks[i:i + BATCH_SIZE]\n",
    "                logger.info(f\"Processing batch {i//BATCH_SIZE + 1}/{(len(all_chunks) + BATCH_SIZE - 1)//BATCH_SIZE}\")\n",
    "                \n",
    "                try:\n",
    "                    # Extract text content for embedding generation\n",
    "                    texts = [chunk.page_content for chunk in batch_chunks]\n",
    "                    \n",
    "                    # Generate embeddings for the batch\n",
    "                    batch_embeddings = self.generate_embeddings_batch(texts)\n",
    "                    \n",
    "                    # Prepare data for insertion\n",
    "                    batch_data = self.prepare_batch_data(batch_chunks)\n",
    "                    \n",
    "                    # Insert batch into database\n",
    "                    self.insert_embeddings_batch(cursor, batch_data, batch_embeddings)\n",
    "                    conn.commit()\n",
    "                    \n",
    "                    logger.info(f\"Successfully processed batch {i//BATCH_SIZE + 1}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing batch {i//BATCH_SIZE + 1}: {e}\")\n",
    "                    conn.rollback()\n",
    "                    raise\n",
    "        \n",
    "        logger.info(\"All document chunks processed successfully\")\n",
    "\n",
    "# Usage example\n",
    "def process_vector_embeddings(connection_string,table_name,chunks):\n",
    "    # Initialize the processor\n",
    "    processor = VectorEmbeddingsProcessor(connection_string,table_name)    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        processor.process_documents(chunks)\n",
    "        print(\"Embeddings processing completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process embeddings: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "00c3b414-5e65-4035-a0da-fadba50920d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "11cc524b-4a67-4484-a9d8-a4e298398bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing 193 document chunks\n",
      "INFO:__main__:Processing batch 1/2\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Successfully processed batch 1\n",
      "INFO:__main__:Processing batch 2/2\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Successfully processed batch 2\n",
      "INFO:__main__:All document chunks processed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "CONNECTION_STRING = \"host=localhost port=5432 dbname=vector_db user=vector password=vector\"\n",
    "table_name = \"public.t_document_embeddings\"\n",
    "process_vector_embeddings(CONNECTION_STRING,table_name,chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "a4c5b4a9-5067-4eb7-acf3-3ab80364e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def retrieve_context(table_name,file_name,connection_string,question,top_k):\n",
    "    # Correct DSN    \n",
    "    # OpenAI Embeddings\n",
    "    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    embedding = embedding_function.embed_query(question)\n",
    "\n",
    "    \n",
    "    query = f\"\"\"SELECT chunk, embedding <-> '{embedding}' AS distance --<#> for inner product, and <-> for cosine distance\n",
    "    FROM {table_name}\n",
    "    where file_name = '{file_name}'\n",
    "    ORDER BY distance\n",
    "    LIMIT {top_k};\"\"\"\n",
    "\n",
    "    print(query)\n",
    "    \n",
    "    conn = psycopg2.connect(connection_string)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    context = \"\\n\".join([row[0] for row in result])\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7a49f-e811-4dd3-a221-df3d536938e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea8c98-6914-4654-93c5-486fd604fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "\n",
    "question = \"who won in sattenapalli constituency?\"\n",
    "\n",
    "\n",
    "CONNECTION_STRING = \"host=localhost port=5432 dbname=vector_db user=vector password=vector\"\n",
    "table_name = \"public.t_document_embeddings\"\n",
    "base_name = os.path.basename(chunks[0].metadata[\"source\"])\n",
    "\n",
    "file_name = re.sub(r'[^a-zA-Z0-9_.-]', '_', base_name)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\",\"\"\"You are a helpful assistant that answers questions based on the context provided. \n",
    "    Use only the given context to answer. If the context does not contain the answer, \n",
    "    say \"I don't know\" and do not make up an answer.\n",
    "    \n",
    "Instructions:\n",
    "- Provide a comprehensive answer based on the context\n",
    "- If relevant, mention which website(s) the information comes from\n",
    "- Be specific and cite details from the context\n",
    "- If the context doesn't contain enough information, clearly state what's missing\n",
    "\n",
    "\"\"\"),\n",
    "                                                    (\"human\",'Context: {context}'),\n",
    "                                                    (\"human\",\"Question: {question}\")])\n",
    "\n",
    "context =  retrieve_context(table_name,file_name,CONNECTION_STRING ,question,3)\n",
    "print(context)\n",
    "\n",
    "rag_chain = ({\"context\": itemgetter(\"context\") ,\n",
    "              \"question\":itemgetter(\"question\") } | prompt  | llm | StrOutputParser() )\n",
    "\n",
    "llm_response = rag_chain.invoke({\"question\":question,\"context\":context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "ba1f952b-4b76-46ff-8fcd-5162ca5b2833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "print(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
